{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes\n",
      "- Pros: Works with a small amount of data, handles multiple classes\n",
      "- Cons: Sensitive to how the input data is prepared\n",
      "- Works with: Nominal values\n",
      "##Keep in Mind\n",
      "\n",
      "- Want the probability of having class $c_i$ given features x,y: $P(c_i|x,y)$\n",
      "- If $P(c_1|x,y) > P(c_2|x,y)$, the class is $c_1$\n",
      "- If $P(c_1|x,y) < P(c_2|x,y)$, the class is $c_2$\n",
      "- Bayes theory gives: $P(c_i|x,y) =  {\\large \\frac{P(x,y|c_i)P(c_i)}{P(x,y)} }$\n",
      "- 'Naive' comes from the fact that we assume that variables are independent which is not true but works in practice, thus we have:\n",
      "- $P(c_i|x,y) =  {\\large \\frac{P(x|c_i)P(y|c_i)P(c_i)}{P(x,y)} }$\n",
      "- how to compute $P(x|c_i)$ is described in the code below\n",
      "- don't need to compute $P(x,y)$ since we only need to compare $P(c_1|x,y)$ and $P(c_2|x,y)$\n",
      "- to avoid having $P(x|c_i)$ == 0 which would make the whole $P(c_i|x,y)$ 0, we start with non zero sums (see `p0Num=ones(numWords); p0Denom=2.0`)\n",
      "- to avoid small numbers multiplication (underflow), we use logs (see `p0Vect = log(p0Num/p0Denom)`) and the fact that `ln(a*b) = ln(a)+ln(b)`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import *\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import operator\n",
      "from os import listdir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 1 : identify abusive posts from a forum"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# returns documents (1 line per document) and vector of classes for each \n",
      "# of these documents\n",
      "def loadDataSet():\n",
      "  postingList=[['my', 'dog', 'has', 'flea', \\\n",
      "     'problems', 'help', 'please'],\n",
      "     ['maybe', 'not', 'take', 'him', \\\n",
      "     'to', 'dog', 'park', 'stupid'],\n",
      "     ['my', 'dalmation', 'is', 'so', 'cute', \\\n",
      "     'I', 'love', 'him'],\n",
      "     ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
      "     ['mr', 'licks', 'ate', 'my', 'steak', 'how',\\\n",
      "     'to', 'stop', 'him'],\n",
      "     ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "  classVec = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
      "  return postingList,classVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create the list of all distinct words in the dataset\n",
      "def createVocabList(dataSet):\n",
      "  vocabSet = set([]) \n",
      "  for document in dataSet:\n",
      "    vocabSet = vocabSet | set(document) \n",
      "  return list(vocabSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listOPosts,listClasses = loadDataSet()\n",
      "listOPosts, [ 'abusive' if cl == 1 else 'normal' for cl in listClasses]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
        "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
        "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
        "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
        "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
        "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
        " ['normal', 'abusive', 'normal', 'abusive', 'normal', 'abusive'])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listOPosts[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bagOfWords2Vec(vocabList, inputSet):\n",
      "  returnVec = [0]*len(vocabList) \n",
      "  for word in inputSet:\n",
      "    if word in vocabList:\n",
      "      returnVec[vocabList.index(word)] += 1\n",
      "    else: print \"the word: %s is not in my Vocabulary!\" % word\n",
      "  return returnVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myVocabList = createVocabList(listOPosts)\n",
      "bagOfWords2Vec(myVocabList, listOPosts[0]) [0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainNB0(trainMatrix,trainCategory):\n",
      "  numTrainDocs = len(trainMatrix)\n",
      "  numWords = len(trainMatrix[0])\n",
      "  # proportion of abusive doc on the total number of docs\n",
      "  pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
      "  p0Num = ones(numWords); p1Num = ones(numWords) \n",
      "  p0Denom = 2.0; p1Denom = 2.0 \n",
      "  for i in range(numTrainDocs):\n",
      "    if trainCategory[i] == 1:\n",
      "      p1Num += trainMatrix[i]        # add to vector of words for class 1\n",
      "      p1Denom += sum(trainMatrix[i]) # add to total number of words for class 1\n",
      "    else:\n",
      "      p0Num += trainMatrix[i]\n",
      "      p0Denom += sum(trainMatrix[i])\n",
      "  p1Vect = log(p1Num/p1Denom)\n",
      "  p0Vect = log(p0Num/p0Denom)\n",
      "  return p0Vect,p1Vect,pAbusive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
      "  p1 = sum(vec2Classify * p1Vec) + log(pClass1) \n",
      "  p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
      "  if p1 > p0:\n",
      "    return 1\n",
      "  else: \n",
      "    return 0\n",
      "\n",
      "def testingNB():\n",
      "  listOPosts,listClasses = loadDataSet()\n",
      "  myVocabList = createVocabList(listOPosts)\n",
      "  trainMat=[]\n",
      "  for postinDoc in listOPosts:\n",
      "    trainMat.append(bagOfWords2Vec(myVocabList, postinDoc))\n",
      "  p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
      "  testEntry = ['love', 'my', 'dalmation']\n",
      "  thisDoc = array(bagOfWords2Vec(myVocabList, testEntry))\n",
      "  print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
      "  testEntry = ['stupid', 'garbage']\n",
      "  thisDoc = array(bagOfWords2Vec(myVocabList, testEntry))\n",
      "  print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testingNB()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['love', 'my', 'dalmation'] classified as:  0\n",
        "['stupid', 'garbage'] classified as:  1\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 2: classify spam email"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def textParse(bigString): \n",
      "  import re\n",
      "  regex = re.compile('\\\\W*')\n",
      "  return [w.lower() for w in regex.split(bigString) if len(w) > 2]\n",
      "\n",
      "def spamTest():\n",
      "  docList=[]; classList = []; fullText =[]\n",
      "  for i in range(1,26):\n",
      "    wordList = textParse(open('email/spam/%d.txt' % i).read()) \n",
      "    docList.append(wordList) \n",
      "    fullText.extend(wordList) \n",
      "    classList.append(1) \n",
      "    wordList = textParse(open('email/ham/%d.txt' % i).read()) \n",
      "    docList.append(wordList) \n",
      "    fullText.extend(wordList) \n",
      "    classList.append(0)\n",
      "  vocabList = createVocabList(docList)\n",
      "  trainingSet = range(50); testSet=[] \n",
      "  for i in range(10): \n",
      "    randIndex = int(random.uniform(0,len(trainingSet))) \n",
      "    testSet.append(trainingSet[randIndex]) \n",
      "    del(trainingSet[randIndex]) \n",
      "  trainMat=[]; trainClasses = []\n",
      "  for docIndex in trainingSet:\n",
      "    trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))\n",
      "    trainClasses.append(classList[docIndex])\n",
      "  p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
      "  errorCount = 0\n",
      "  for docIndex in testSet: \n",
      "    wordVector = bagOfWords2Vec(vocabList, docList[docIndex]) \n",
      "    if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: \n",
      "      errorCount += 1\n",
      "  print 'the error rate is: ',float(errorCount)/len(testSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# redo this several times and compute an average. Should obtain about 0.06.\n",
      "spamTest()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the error rate is:  0.1\n"
       ]
      }
     ],
     "prompt_number": 24
    }
   ],
   "metadata": {}
  }
 ]
}